{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-4B_ZHG0Mv9",
        "outputId": "57eda97d-5e04-4834-b320-455135a6ad87"
      },
      "id": "h-4B_ZHG0Mv9",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "x0OjKYcW2a9s"
      },
      "id": "x0OjKYcW2a9s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "28fc7fd9",
      "metadata": {
        "id": "28fc7fd9"
      },
      "source": [
        "Import the python libraries necessary for creating a Naive Bayes supervised machine learning algorithm used to classify email spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "74a9864c",
      "metadata": {
        "id": "74a9864c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb3703ff",
      "metadata": {
        "id": "eb3703ff"
      },
      "source": [
        "Create an \"extract features\" function to clean and prepare the data for the model. This entails removing the non required words, expressions and symbols from the text. The end result will be a matrix representation of the word frequency across each document, each item in the dictionary represented as a column and each email represented by a row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "16167890",
      "metadata": {
        "id": "16167890"
      },
      "outputs": [],
      "source": [
        "# define the function that will extract features and create the dictionary\n",
        "def make_Dictionary(root_dir):\n",
        "# create an empty list\n",
        "  all_words = []\n",
        "# create the emails master list by combining the email paths into one single path\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
        "# initiate a for loop to loop through each email\n",
        "  for mail in emails:\n",
        "# assign m to each email\n",
        "    with open(mail) as m:\n",
        "# initiate another for loop to loop through each line in m (the email), then split the words in the line, then add all the strings in words to all_words\n",
        "      for line in m:\n",
        "        words = line.split()\n",
        "        all_words += words\n",
        "# define the dictionary and fill it up by counting the number of occurences of each item (word) in all_words\n",
        "  dictionary = Counter(all_words)\n",
        "# create a list of iterables within dictionary (words) and assign the list to list_to_remove\n",
        "  list_to_remove = list(dictionary)\n",
        "#initiate a for loop for every item (words) in list_to_remove\n",
        "  for item in list_to_remove:\n",
        "# isalpha() returns “True” if all characters in the string are alphabetical (in [a-z or A-Z]) - else (if there are numbers or special characters), it returns “False”\n",
        "    if item.isalpha() == False:\n",
        "# if the string contains anything other than letters, delete the item from the dictionary\n",
        "      del dictionary[item]\n",
        "# else, if the string only is one letter long, delete the item from the dictionary\n",
        "    elif len(item) == 1:\n",
        "      del dictionary[item]\n",
        "# keep only the most common 3,000 items in the dictionary\n",
        "  dictionary = dictionary.most_common(3000)\n",
        "# return the dictionary as the result of the first for loop\n",
        "  return dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74b0eb7f",
      "metadata": {
        "id": "74b0eb7f"
      },
      "source": [
        "This function will create feature columns and populate their value for each email. The Feature Matrix will have 3000 columns, representing the 3000 most common words we kept in the dictionary, and either 702 (training) or 260 (test) rows representing the number of email files.\n",
        "The function also analyzes the name of each email file and decides if it's spam or not based on whether the filepath contains 'spmsg'. The function also creates a new column, Labelled Data Column, that labels the email as spam or not. \n",
        "This function is used to gather the training dataset, gather the emails for the testing dataset, and returns the feature dataset with the New Label column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "73885346",
      "metadata": {
        "id": "73885346"
      },
      "outputs": [],
      "source": [
        "# define the function that will create the docterm matrix from the mail_dir \n",
        "def extract_features(mail_dir):\n",
        "# use a for loop to join together the files using os.listdir to grab all files and directories in mail_dir and os.path.join to concatenate these path components\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
        "# create an array, filled with zeros in every place, with 3000 columns (for the 3000 most common words) and how many ever rows are in the mail_dir for the selected data\n",
        "  features_matrix = np.zeros((len(files),3000))\n",
        "# create a train labels array filled with zeros, the array will be one row with columns the length of the amount of files in the selected data\n",
        "  train_labels = np.zeros(len(files))\n",
        "# set the count equal to 1 to keep track of how many times the algorithm has iterated\n",
        "  count = 1;\n",
        "  docID = 0;\n",
        "# initiate another for loop that will eventually tokenize the \n",
        "  for fil in files:\n",
        "# open fil and return it as a file object\n",
        "    with open(fil) as fi:\n",
        "# for each element in fi, a tuple is produced with count, docID; the for loop binds that to i, line\n",
        "      for i, line in enumerate(fi):\n",
        "# if i exactly equals 2, then\n",
        "        if i == 2:\n",
        "# split line into a list\n",
        "          words = line.split()\n",
        "# initiate a for loop to iterate through each word in words\n",
        "          for word in words:\n",
        "# set wordID to zero\n",
        "            wordID = 0\n",
        "# initiate a for loop that will count the amount of times the word appears\n",
        "            for i, d in enumerate(dictionary):\n",
        "              if d[0] == word:\n",
        "                wordID = i\n",
        "                features_matrix[docID,wordID] = words.count(word)\n",
        "# train the program to recognize whether or not the email is a spam email\n",
        "      train_labels[docID] = 0;\n",
        "#separate the first part of the file name before the '/'\n",
        "      filepathTokens = fil.split('/')\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
        "# if the first part starts with spmsg\n",
        "      if lastToken.startswith(\"spmsg\"):\n",
        "# the train column of this particular docID is 1, meaning yes it's spam\n",
        "        train_labels[docID] = 1;\n",
        "# keep track of how many times the loop has iterated\n",
        "        count = count + 1\n",
        "# keep track of the document ID\n",
        "      docID = docID + 1\n",
        "# when the function is called, return the matrix with the train column\n",
        "  return features_matrix, train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06666c90",
      "metadata": {
        "id": "06666c90"
      },
      "source": [
        "Call the two functions created above. First, we train the model using the model.fit function and Training Dataset. Then it evaluates the accuracy of the trained model by comparing it to the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fc3bb74e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc3bb74e",
        "outputId": "8f46f751-d60c-44bf-d330-17d1417ebc8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ]
        }
      ],
      "source": [
        "TRAIN_DIR = '/content/drive/My Drive/BSAN6070/Data/train-mails'\n",
        "TEST_DIR = '/content/drive/My Drive/BSAN6070/Data/test-mails'\n",
        "\n",
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print (\"Reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm\")\n",
        "model.fit(features_matrix, labels)\n",
        "print (\"Training completed\")\n",
        "print (\"Testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (\"The accuracy score of this model is \", accuracy_score(test_labels, predicted_labels))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "CA02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}